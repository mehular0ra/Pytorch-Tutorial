{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch Linear Classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNjKjxfCUfl/pfCEmQ8XO8i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehular0ra/Pytorch-Tutorial/blob/main/Pytorch_Linear_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dfFfz1LI7dK4"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load in the data\n",
        "from sklearn.datasets import load_breast_cancer"
      ],
      "metadata": {
        "id": "NiT6oCmr8Bnv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "data = load_breast_cancer()"
      ],
      "metadata": {
        "id": "HPvFxyQk8Ug3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQYRy6TG-Lm9",
        "outputId": "0187db28-dbf5-4831-ddaa-74f22029799c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "sklearn.utils.Bunch"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcKFYEMk-Mf5",
        "outputId": "317584b5-f532-459d-a59a-012e0e9ccad4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUYh0eLg-OXS",
        "outputId": "3b565f75-b27c-4fdb-bd8b-dc40b6b0cd1d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.target)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkGQuqce-hCF",
        "outputId": "9b8bb1bc-2550-46bb-b319-cb7d1cd391fe"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
            " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
            " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
            " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
            " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
            " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
            " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
            " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
            " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
            " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.target_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4PRMAfC-ikV",
        "outputId": "af40a15a-ae50-45c2-b960-9e32a3282a62"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['malignant', 'benign'], dtype='<U9')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.target.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WEDBc3Q-xGJ",
        "outputId": "14020f62-fb89-434e-f3da-8d70fb0cbd9b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569,)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.feature_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMt7HfsJ-1g3",
        "outputId": "68623d5d-c914-4cf4-f772-5228ca45e4cb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
              "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
              "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
              "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
              "       'smoothness error', 'compactness error', 'concavity error',\n",
              "       'concave points error', 'symmetry error',\n",
              "       'fractal dimension error', 'worst radius', 'worst texture',\n",
              "       'worst perimeter', 'worst area', 'worst smoothness',\n",
              "       'worst compactness', 'worst concavity', 'worst concave points',\n",
              "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(load_breast_cancer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vr8V__st-7XC",
        "outputId": "e1dc92fb-ea5e-4ab0-f367-f5fce0217e1b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function load_breast_cancer in module sklearn.datasets._base:\n",
            "\n",
            "load_breast_cancer(*, return_X_y=False, as_frame=False)\n",
            "    Load and return the breast cancer wisconsin dataset (classification).\n",
            "    \n",
            "    The breast cancer dataset is a classic and very easy binary classification\n",
            "    dataset.\n",
            "    \n",
            "    =================   ==============\n",
            "    Classes                          2\n",
            "    Samples per class    212(M),357(B)\n",
            "    Samples total                  569\n",
            "    Dimensionality                  30\n",
            "    Features            real, positive\n",
            "    =================   ==============\n",
            "    \n",
            "    Read more in the :ref:`User Guide <breast_cancer_dataset>`.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    return_X_y : bool, default=False\n",
            "        If True, returns ``(data, target)`` instead of a Bunch object.\n",
            "        See below for more information about the `data` and `target` object.\n",
            "    \n",
            "        .. versionadded:: 0.18\n",
            "    \n",
            "    as_frame : bool, default=False\n",
            "        If True, the data is a pandas DataFrame including columns with\n",
            "        appropriate dtypes (numeric). The target is\n",
            "        a pandas DataFrame or Series depending on the number of target columns.\n",
            "        If `return_X_y` is True, then (`data`, `target`) will be pandas\n",
            "        DataFrames or Series as described below.\n",
            "    \n",
            "        .. versionadded:: 0.23\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    data : :class:`~sklearn.utils.Bunch`\n",
            "        Dictionary-like object, with the following attributes.\n",
            "    \n",
            "        data : {ndarray, dataframe} of shape (569, 30)\n",
            "            The data matrix. If `as_frame=True`, `data` will be a pandas\n",
            "            DataFrame.\n",
            "        target: {ndarray, Series} of shape (569,)\n",
            "            The classification target. If `as_frame=True`, `target` will be\n",
            "            a pandas Series.\n",
            "        feature_names: list\n",
            "            The names of the dataset columns.\n",
            "        target_names: list\n",
            "            The names of target classes.\n",
            "        frame: DataFrame of shape (569, 31)\n",
            "            Only present when `as_frame=True`. DataFrame with `data` and\n",
            "            `target`.\n",
            "    \n",
            "            .. versionadded:: 0.23\n",
            "        DESCR: str\n",
            "            The full description of the dataset.\n",
            "        filename: str\n",
            "            The path to the location of the data.\n",
            "    \n",
            "            .. versionadded:: 0.20\n",
            "    \n",
            "    (data, target) : tuple if ``return_X_y`` is True\n",
            "    \n",
            "        .. versionadded:: 0.18\n",
            "    \n",
            "    The copy of UCI ML Breast Cancer Wisconsin (Diagnostic) dataset is\n",
            "    downloaded from:\n",
            "    https://goo.gl/U2Uwz2\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    Let's say you are interested in the samples 10, 50, and 85, and want to\n",
            "    know their class name.\n",
            "    \n",
            "    >>> from sklearn.datasets import load_breast_cancer\n",
            "    >>> data = load_breast_cancer()\n",
            "    >>> data.target[[10, 50, 85]]\n",
            "    array([0, 1, 0])\n",
            "    >>> list(data.target_names)\n",
            "    ['malignant', 'benign']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into train and test set \n",
        "# using sklearn.model_selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.33)"
      ],
      "metadata": {
        "id": "4aEBrDq5CfgO"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# standardize (normalize) data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "N, D = X_train.shape"
      ],
      "metadata": {
        "id": "_dyMfgDNC_La"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build the logistic regression model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(D, 1),\n",
        "    nn.Sigmoid()\n",
        ")"
      ],
      "metadata": {
        "id": "xge5nhifEJCe"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "SBLJLMPcEeZK"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# covert train and test data to float32 format\n",
        "# in pytorch, float32 is by default initialized\n",
        "# in numpy, float64 is by default initialized\n",
        "train_inputs = torch.from_numpy(X_train.astype(np.float32))\n",
        "train_targets = torch.from_numpy(y_train.astype(np.float32)).reshape(-1, 1)\n",
        "\n",
        "test_inputs = torch.from_numpy(X_test.astype(np.float32))\n",
        "test_targets = torch.from_numpy(y_test.astype(np.float32)).reshape(-1, 1)\n"
      ],
      "metadata": {
        "id": "zBfxedaRFmDe"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 2000\n",
        "\n",
        "# save the train and test losses\n",
        "train_losses = np.zeros(n_epochs)\n",
        "test_losses = np.zeros(n_epochs)\n",
        "\n",
        "for it in range(n_epochs):\n",
        "    # zero the gradient parameters\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # feed-forward (on training data)\n",
        "    train_outputs = model(train_inputs)\n",
        "    train_loss = criterion(train_outputs, train_targets)\n",
        "\n",
        "    # backward and optimize\n",
        "    train_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # get the test loss\n",
        "    test_outputs = model(test_inputs)\n",
        "    test_loss = criterion(test_outputs, test_targets)\n",
        "\n",
        "    # save the losses\n",
        "    train_losses[it] = train_loss.item()\n",
        "    test_losses[it] = test_loss.item()\n",
        "\n",
        "    # print the epoch number and the loss at that epoch at 5th iteration\n",
        "    if (it+1) % 10  == 0:\n",
        "        print(f'Epoch: {it+1}/{n_epochs}, Train loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
        "\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3rOeIGSFJHt",
        "outputId": "032a5831-c656-4de4-92a7-a98dcf091e74"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10/2000, Train loss: 0.5381, Test Loss: 0.5490\n",
            "Epoch: 20/2000, Train loss: 0.4912, Test Loss: 0.5061\n",
            "Epoch: 30/2000, Train loss: 0.4518, Test Loss: 0.4695\n",
            "Epoch: 40/2000, Train loss: 0.4186, Test Loss: 0.4381\n",
            "Epoch: 50/2000, Train loss: 0.3905, Test Loss: 0.4108\n",
            "Epoch: 60/2000, Train loss: 0.3664, Test Loss: 0.3868\n",
            "Epoch: 70/2000, Train loss: 0.3455, Test Loss: 0.3656\n",
            "Epoch: 80/2000, Train loss: 0.3272, Test Loss: 0.3468\n",
            "Epoch: 90/2000, Train loss: 0.3110, Test Loss: 0.3301\n",
            "Epoch: 100/2000, Train loss: 0.2966, Test Loss: 0.3151\n",
            "Epoch: 110/2000, Train loss: 0.2836, Test Loss: 0.3015\n",
            "Epoch: 120/2000, Train loss: 0.2719, Test Loss: 0.2892\n",
            "Epoch: 130/2000, Train loss: 0.2613, Test Loss: 0.2779\n",
            "Epoch: 140/2000, Train loss: 0.2516, Test Loss: 0.2676\n",
            "Epoch: 150/2000, Train loss: 0.2428, Test Loss: 0.2581\n",
            "Epoch: 160/2000, Train loss: 0.2346, Test Loss: 0.2493\n",
            "Epoch: 170/2000, Train loss: 0.2271, Test Loss: 0.2411\n",
            "Epoch: 180/2000, Train loss: 0.2202, Test Loss: 0.2336\n",
            "Epoch: 190/2000, Train loss: 0.2138, Test Loss: 0.2265\n",
            "Epoch: 200/2000, Train loss: 0.2078, Test Loss: 0.2200\n",
            "Epoch: 210/2000, Train loss: 0.2022, Test Loss: 0.2139\n",
            "Epoch: 220/2000, Train loss: 0.1970, Test Loss: 0.2081\n",
            "Epoch: 230/2000, Train loss: 0.1922, Test Loss: 0.2028\n",
            "Epoch: 240/2000, Train loss: 0.1876, Test Loss: 0.1977\n",
            "Epoch: 250/2000, Train loss: 0.1833, Test Loss: 0.1930\n",
            "Epoch: 260/2000, Train loss: 0.1793, Test Loss: 0.1885\n",
            "Epoch: 270/2000, Train loss: 0.1755, Test Loss: 0.1843\n",
            "Epoch: 280/2000, Train loss: 0.1720, Test Loss: 0.1804\n",
            "Epoch: 290/2000, Train loss: 0.1686, Test Loss: 0.1766\n",
            "Epoch: 300/2000, Train loss: 0.1654, Test Loss: 0.1731\n",
            "Epoch: 310/2000, Train loss: 0.1624, Test Loss: 0.1697\n",
            "Epoch: 320/2000, Train loss: 0.1595, Test Loss: 0.1666\n",
            "Epoch: 330/2000, Train loss: 0.1568, Test Loss: 0.1635\n",
            "Epoch: 340/2000, Train loss: 0.1542, Test Loss: 0.1607\n",
            "Epoch: 350/2000, Train loss: 0.1518, Test Loss: 0.1579\n",
            "Epoch: 360/2000, Train loss: 0.1494, Test Loss: 0.1553\n",
            "Epoch: 370/2000, Train loss: 0.1472, Test Loss: 0.1529\n",
            "Epoch: 380/2000, Train loss: 0.1450, Test Loss: 0.1505\n",
            "Epoch: 390/2000, Train loss: 0.1430, Test Loss: 0.1482\n",
            "Epoch: 400/2000, Train loss: 0.1410, Test Loss: 0.1461\n",
            "Epoch: 410/2000, Train loss: 0.1391, Test Loss: 0.1440\n",
            "Epoch: 420/2000, Train loss: 0.1373, Test Loss: 0.1420\n",
            "Epoch: 430/2000, Train loss: 0.1356, Test Loss: 0.1401\n",
            "Epoch: 440/2000, Train loss: 0.1339, Test Loss: 0.1383\n",
            "Epoch: 450/2000, Train loss: 0.1323, Test Loss: 0.1365\n",
            "Epoch: 460/2000, Train loss: 0.1308, Test Loss: 0.1348\n",
            "Epoch: 470/2000, Train loss: 0.1293, Test Loss: 0.1332\n",
            "Epoch: 480/2000, Train loss: 0.1279, Test Loss: 0.1316\n",
            "Epoch: 490/2000, Train loss: 0.1265, Test Loss: 0.1301\n",
            "Epoch: 500/2000, Train loss: 0.1252, Test Loss: 0.1286\n",
            "Epoch: 510/2000, Train loss: 0.1239, Test Loss: 0.1272\n",
            "Epoch: 520/2000, Train loss: 0.1226, Test Loss: 0.1258\n",
            "Epoch: 530/2000, Train loss: 0.1214, Test Loss: 0.1245\n",
            "Epoch: 540/2000, Train loss: 0.1203, Test Loss: 0.1232\n",
            "Epoch: 550/2000, Train loss: 0.1191, Test Loss: 0.1220\n",
            "Epoch: 560/2000, Train loss: 0.1180, Test Loss: 0.1208\n",
            "Epoch: 570/2000, Train loss: 0.1170, Test Loss: 0.1196\n",
            "Epoch: 580/2000, Train loss: 0.1160, Test Loss: 0.1185\n",
            "Epoch: 590/2000, Train loss: 0.1150, Test Loss: 0.1174\n",
            "Epoch: 600/2000, Train loss: 0.1140, Test Loss: 0.1163\n",
            "Epoch: 610/2000, Train loss: 0.1130, Test Loss: 0.1153\n",
            "Epoch: 620/2000, Train loss: 0.1121, Test Loss: 0.1143\n",
            "Epoch: 630/2000, Train loss: 0.1112, Test Loss: 0.1133\n",
            "Epoch: 640/2000, Train loss: 0.1104, Test Loss: 0.1124\n",
            "Epoch: 650/2000, Train loss: 0.1095, Test Loss: 0.1115\n",
            "Epoch: 660/2000, Train loss: 0.1087, Test Loss: 0.1106\n",
            "Epoch: 670/2000, Train loss: 0.1079, Test Loss: 0.1097\n",
            "Epoch: 680/2000, Train loss: 0.1071, Test Loss: 0.1089\n",
            "Epoch: 690/2000, Train loss: 0.1064, Test Loss: 0.1080\n",
            "Epoch: 700/2000, Train loss: 0.1056, Test Loss: 0.1072\n",
            "Epoch: 710/2000, Train loss: 0.1049, Test Loss: 0.1064\n",
            "Epoch: 720/2000, Train loss: 0.1042, Test Loss: 0.1057\n",
            "Epoch: 730/2000, Train loss: 0.1035, Test Loss: 0.1049\n",
            "Epoch: 740/2000, Train loss: 0.1028, Test Loss: 0.1042\n",
            "Epoch: 750/2000, Train loss: 0.1022, Test Loss: 0.1035\n",
            "Epoch: 760/2000, Train loss: 0.1016, Test Loss: 0.1028\n",
            "Epoch: 770/2000, Train loss: 0.1009, Test Loss: 0.1021\n",
            "Epoch: 780/2000, Train loss: 0.1003, Test Loss: 0.1014\n",
            "Epoch: 790/2000, Train loss: 0.0997, Test Loss: 0.1008\n",
            "Epoch: 800/2000, Train loss: 0.0991, Test Loss: 0.1001\n",
            "Epoch: 810/2000, Train loss: 0.0985, Test Loss: 0.0995\n",
            "Epoch: 820/2000, Train loss: 0.0980, Test Loss: 0.0989\n",
            "Epoch: 830/2000, Train loss: 0.0974, Test Loss: 0.0983\n",
            "Epoch: 840/2000, Train loss: 0.0969, Test Loss: 0.0977\n",
            "Epoch: 850/2000, Train loss: 0.0964, Test Loss: 0.0971\n",
            "Epoch: 860/2000, Train loss: 0.0958, Test Loss: 0.0966\n",
            "Epoch: 870/2000, Train loss: 0.0953, Test Loss: 0.0960\n",
            "Epoch: 880/2000, Train loss: 0.0948, Test Loss: 0.0955\n",
            "Epoch: 890/2000, Train loss: 0.0944, Test Loss: 0.0949\n",
            "Epoch: 900/2000, Train loss: 0.0939, Test Loss: 0.0944\n",
            "Epoch: 910/2000, Train loss: 0.0934, Test Loss: 0.0939\n",
            "Epoch: 920/2000, Train loss: 0.0929, Test Loss: 0.0934\n",
            "Epoch: 930/2000, Train loss: 0.0925, Test Loss: 0.0929\n",
            "Epoch: 940/2000, Train loss: 0.0920, Test Loss: 0.0924\n",
            "Epoch: 950/2000, Train loss: 0.0916, Test Loss: 0.0920\n",
            "Epoch: 960/2000, Train loss: 0.0912, Test Loss: 0.0915\n",
            "Epoch: 970/2000, Train loss: 0.0908, Test Loss: 0.0911\n",
            "Epoch: 980/2000, Train loss: 0.0903, Test Loss: 0.0906\n",
            "Epoch: 990/2000, Train loss: 0.0899, Test Loss: 0.0902\n",
            "Epoch: 1000/2000, Train loss: 0.0895, Test Loss: 0.0897\n",
            "Epoch: 1010/2000, Train loss: 0.0892, Test Loss: 0.0893\n",
            "Epoch: 1020/2000, Train loss: 0.0888, Test Loss: 0.0889\n",
            "Epoch: 1030/2000, Train loss: 0.0884, Test Loss: 0.0885\n",
            "Epoch: 1040/2000, Train loss: 0.0880, Test Loss: 0.0881\n",
            "Epoch: 1050/2000, Train loss: 0.0876, Test Loss: 0.0877\n",
            "Epoch: 1060/2000, Train loss: 0.0873, Test Loss: 0.0873\n",
            "Epoch: 1070/2000, Train loss: 0.0869, Test Loss: 0.0869\n",
            "Epoch: 1080/2000, Train loss: 0.0866, Test Loss: 0.0865\n",
            "Epoch: 1090/2000, Train loss: 0.0862, Test Loss: 0.0862\n",
            "Epoch: 1100/2000, Train loss: 0.0859, Test Loss: 0.0858\n",
            "Epoch: 1110/2000, Train loss: 0.0856, Test Loss: 0.0854\n",
            "Epoch: 1120/2000, Train loss: 0.0852, Test Loss: 0.0851\n",
            "Epoch: 1130/2000, Train loss: 0.0849, Test Loss: 0.0848\n",
            "Epoch: 1140/2000, Train loss: 0.0846, Test Loss: 0.0844\n",
            "Epoch: 1150/2000, Train loss: 0.0843, Test Loss: 0.0841\n",
            "Epoch: 1160/2000, Train loss: 0.0840, Test Loss: 0.0837\n",
            "Epoch: 1170/2000, Train loss: 0.0837, Test Loss: 0.0834\n",
            "Epoch: 1180/2000, Train loss: 0.0834, Test Loss: 0.0831\n",
            "Epoch: 1190/2000, Train loss: 0.0831, Test Loss: 0.0828\n",
            "Epoch: 1200/2000, Train loss: 0.0828, Test Loss: 0.0825\n",
            "Epoch: 1210/2000, Train loss: 0.0825, Test Loss: 0.0822\n",
            "Epoch: 1220/2000, Train loss: 0.0822, Test Loss: 0.0819\n",
            "Epoch: 1230/2000, Train loss: 0.0819, Test Loss: 0.0816\n",
            "Epoch: 1240/2000, Train loss: 0.0816, Test Loss: 0.0813\n",
            "Epoch: 1250/2000, Train loss: 0.0814, Test Loss: 0.0810\n",
            "Epoch: 1260/2000, Train loss: 0.0811, Test Loss: 0.0807\n",
            "Epoch: 1270/2000, Train loss: 0.0808, Test Loss: 0.0804\n",
            "Epoch: 1280/2000, Train loss: 0.0806, Test Loss: 0.0801\n",
            "Epoch: 1290/2000, Train loss: 0.0803, Test Loss: 0.0799\n",
            "Epoch: 1300/2000, Train loss: 0.0800, Test Loss: 0.0796\n",
            "Epoch: 1310/2000, Train loss: 0.0798, Test Loss: 0.0793\n",
            "Epoch: 1320/2000, Train loss: 0.0795, Test Loss: 0.0791\n",
            "Epoch: 1330/2000, Train loss: 0.0793, Test Loss: 0.0788\n",
            "Epoch: 1340/2000, Train loss: 0.0791, Test Loss: 0.0786\n",
            "Epoch: 1350/2000, Train loss: 0.0788, Test Loss: 0.0783\n",
            "Epoch: 1360/2000, Train loss: 0.0786, Test Loss: 0.0781\n",
            "Epoch: 1370/2000, Train loss: 0.0783, Test Loss: 0.0778\n",
            "Epoch: 1380/2000, Train loss: 0.0781, Test Loss: 0.0776\n",
            "Epoch: 1390/2000, Train loss: 0.0779, Test Loss: 0.0773\n",
            "Epoch: 1400/2000, Train loss: 0.0777, Test Loss: 0.0771\n",
            "Epoch: 1410/2000, Train loss: 0.0774, Test Loss: 0.0769\n",
            "Epoch: 1420/2000, Train loss: 0.0772, Test Loss: 0.0766\n",
            "Epoch: 1430/2000, Train loss: 0.0770, Test Loss: 0.0764\n",
            "Epoch: 1440/2000, Train loss: 0.0768, Test Loss: 0.0762\n",
            "Epoch: 1450/2000, Train loss: 0.0766, Test Loss: 0.0760\n",
            "Epoch: 1460/2000, Train loss: 0.0764, Test Loss: 0.0758\n",
            "Epoch: 1470/2000, Train loss: 0.0761, Test Loss: 0.0755\n",
            "Epoch: 1480/2000, Train loss: 0.0759, Test Loss: 0.0753\n",
            "Epoch: 1490/2000, Train loss: 0.0757, Test Loss: 0.0751\n",
            "Epoch: 1500/2000, Train loss: 0.0755, Test Loss: 0.0749\n",
            "Epoch: 1510/2000, Train loss: 0.0753, Test Loss: 0.0747\n",
            "Epoch: 1520/2000, Train loss: 0.0751, Test Loss: 0.0745\n",
            "Epoch: 1530/2000, Train loss: 0.0749, Test Loss: 0.0743\n",
            "Epoch: 1540/2000, Train loss: 0.0748, Test Loss: 0.0741\n",
            "Epoch: 1550/2000, Train loss: 0.0746, Test Loss: 0.0739\n",
            "Epoch: 1560/2000, Train loss: 0.0744, Test Loss: 0.0738\n",
            "Epoch: 1570/2000, Train loss: 0.0742, Test Loss: 0.0736\n",
            "Epoch: 1580/2000, Train loss: 0.0740, Test Loss: 0.0734\n",
            "Epoch: 1590/2000, Train loss: 0.0738, Test Loss: 0.0732\n",
            "Epoch: 1600/2000, Train loss: 0.0736, Test Loss: 0.0730\n",
            "Epoch: 1610/2000, Train loss: 0.0735, Test Loss: 0.0728\n",
            "Epoch: 1620/2000, Train loss: 0.0733, Test Loss: 0.0727\n",
            "Epoch: 1630/2000, Train loss: 0.0731, Test Loss: 0.0725\n",
            "Epoch: 1640/2000, Train loss: 0.0729, Test Loss: 0.0723\n",
            "Epoch: 1650/2000, Train loss: 0.0728, Test Loss: 0.0722\n",
            "Epoch: 1660/2000, Train loss: 0.0726, Test Loss: 0.0720\n",
            "Epoch: 1670/2000, Train loss: 0.0724, Test Loss: 0.0718\n",
            "Epoch: 1680/2000, Train loss: 0.0723, Test Loss: 0.0717\n",
            "Epoch: 1690/2000, Train loss: 0.0721, Test Loss: 0.0715\n",
            "Epoch: 1700/2000, Train loss: 0.0719, Test Loss: 0.0714\n",
            "Epoch: 1710/2000, Train loss: 0.0718, Test Loss: 0.0712\n",
            "Epoch: 1720/2000, Train loss: 0.0716, Test Loss: 0.0710\n",
            "Epoch: 1730/2000, Train loss: 0.0714, Test Loss: 0.0709\n",
            "Epoch: 1740/2000, Train loss: 0.0713, Test Loss: 0.0707\n",
            "Epoch: 1750/2000, Train loss: 0.0711, Test Loss: 0.0706\n",
            "Epoch: 1760/2000, Train loss: 0.0710, Test Loss: 0.0705\n",
            "Epoch: 1770/2000, Train loss: 0.0708, Test Loss: 0.0703\n",
            "Epoch: 1780/2000, Train loss: 0.0707, Test Loss: 0.0702\n",
            "Epoch: 1790/2000, Train loss: 0.0705, Test Loss: 0.0700\n",
            "Epoch: 1800/2000, Train loss: 0.0704, Test Loss: 0.0699\n",
            "Epoch: 1810/2000, Train loss: 0.0702, Test Loss: 0.0698\n",
            "Epoch: 1820/2000, Train loss: 0.0701, Test Loss: 0.0696\n",
            "Epoch: 1830/2000, Train loss: 0.0699, Test Loss: 0.0695\n",
            "Epoch: 1840/2000, Train loss: 0.0698, Test Loss: 0.0694\n",
            "Epoch: 1850/2000, Train loss: 0.0696, Test Loss: 0.0692\n",
            "Epoch: 1860/2000, Train loss: 0.0695, Test Loss: 0.0691\n",
            "Epoch: 1870/2000, Train loss: 0.0694, Test Loss: 0.0690\n",
            "Epoch: 1880/2000, Train loss: 0.0692, Test Loss: 0.0689\n",
            "Epoch: 1890/2000, Train loss: 0.0691, Test Loss: 0.0688\n",
            "Epoch: 1900/2000, Train loss: 0.0689, Test Loss: 0.0686\n",
            "Epoch: 1910/2000, Train loss: 0.0688, Test Loss: 0.0685\n",
            "Epoch: 1920/2000, Train loss: 0.0687, Test Loss: 0.0684\n",
            "Epoch: 1930/2000, Train loss: 0.0685, Test Loss: 0.0683\n",
            "Epoch: 1940/2000, Train loss: 0.0684, Test Loss: 0.0682\n",
            "Epoch: 1950/2000, Train loss: 0.0683, Test Loss: 0.0681\n",
            "Epoch: 1960/2000, Train loss: 0.0681, Test Loss: 0.0680\n",
            "Epoch: 1970/2000, Train loss: 0.0680, Test Loss: 0.0679\n",
            "Epoch: 1980/2000, Train loss: 0.0679, Test Loss: 0.0677\n",
            "Epoch: 1990/2000, Train loss: 0.0678, Test Loss: 0.0676\n",
            "Epoch: 2000/2000, Train loss: 0.0676, Test Loss: 0.0675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_losses, label='train loss')\n",
        "plt.plot(test_losses, label='test loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "Bl4h2s1XJKpC",
        "outputId": "ff424fd4-0d73-4904-805d-8003906b34da"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get accuracy\n",
        "with torch.no_grad():\n",
        "  p_train = model(train_inputs)\n",
        "  p_train = np.round(p_train.numpy())\n",
        "  train_acc = np.mean(train_targets.numpy() == p_train)\n",
        "\n",
        "  p_test = model(test_inputs)\n",
        "  p_test = np.round(p_test.numpy())\n",
        "  test_acc = np.mean(test_targets.numpy() == p_test)\n",
        "\n",
        "print(f\"Train acc: {train_acc:.4f}, Test acc: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJuhrzZlJTJk",
        "outputId": "5c517889-c826-4161-857a-2a179ecaf8d1"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train acc: 0.9869, Test acc: 0.9787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p_train = model(train_inputs)\n",
        "print(type(p_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8-dUUhVOoSg",
        "outputId": "b791ce23-45d9-4bde-9b8b-8443e1d1a647"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PSw227WXOTBR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}